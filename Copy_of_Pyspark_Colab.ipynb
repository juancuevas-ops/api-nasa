{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Pyspark_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juancuevas-ops/api-nasa/blob/master/Copy_of_Pyspark_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETaCeH1KAcoL"
      },
      "source": [
        "# PySpark en Google Colab Automatico\n",
        "\n",
        "1. Instalacion Marzo/2020\n",
        "2. Instalacion Java\n",
        "3. Instalacion de Spark\n",
        "\n",
        "## Instalacion Marzo/ 2020\n",
        "De forma General para usar pyspark en Colab Marzo/2020 seria con los siguientes comandos en una celda en Colab:\n",
        "```python\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "import os # libreria de manejo del sistema operativo\n",
        "os.system(\"wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\")\n",
        "os.system(\"tar xf /spark-2.4.5-bin-hadoop2.7.tgz\")\n",
        "# instalar pyspark\n",
        "!pip install -q pyspark\n",
        "```\n",
        "\n",
        "```python\n",
        "# Variables de Entorno\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{ver_spark}-bin-hadoop2.7\"\n",
        "```\n",
        "```python\n",
        "# Cargar Pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[*]\").getOrCreate()\n",
        "spark\n",
        "```\n",
        "Pero cuadno salga una nueva version de spark sera necesario actualizar los\n",
        "links de descarga, ya qeu siempre borran las versiones 2.x.x cuando sale una nueva.\n",
        "\n",
        "Lo mejor es configurar automaticamente para que descargue la version que sea\n",
        "mayor que 2.3.4 que es la anterior y menor que spark 3.0.0 que aun se encuentra en desarrollo\n",
        "\n",
        "Para esto el siguiente codigo detecta la version actual de spark, la descarga, la descomprime y luego realiza la instalacion de spark en google colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkGJJVgXBwMa"
      },
      "source": [
        "## Instalacion de Java\n",
        "Google Colaboratory funciona en un ambiente linux, por lo tanto se pueden usar comandos shell de linux antecedidos del caracter '!'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl5KY6D0BH1e"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dHLH3E1BOEk"
      },
      "source": [
        "## Instalacion de Spark\n",
        "\n",
        "Obtener automaticamente la ultima version de spark de "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtThPPGaBYZL"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHfbF3J7CQ8Z"
      },
      "source": [
        "#Obtener las versiones de spark la pagina web\n",
        "url = 'https://downloads.apache.org/spark/' \n",
        "r = requests.get(url)\n",
        "html_doc = r.text\n",
        "soup = BeautifulSoup(html_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnZUvd9XCgqD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c44960a-e73e-437b-93d8-5b5a4951fac2"
      },
      "source": [
        "# leer la pagina web y obtener las versiones de spark disponibles\n",
        "link_files = []\n",
        "for link in soup.find_all('a'):\n",
        "  link_files.append(link.get('href'))\n",
        "spark_link = [x for x in link_files if 'spark' in x]  \n",
        "print(spark_link)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['spark-2.3.4/', 'spark-2.4.5/', 'spark-3.0.0-preview2/']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eciL-34xEx8u"
      },
      "source": [
        "La version a usar seran las superiores a spark-2.3.4  y menores a spark-3.0.0\n",
        "\n",
        "obtener la version y eliminar el caracter '/' del final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY9Fug2LDcMg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a6a31653-ea99-47d8-9bb0-9136e32a49b2"
      },
      "source": [
        "ver_spark = spark_link[1][:-1] # obtener la version y eliminar el caracter '/' del final\n",
        "print(ver_spark)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA3-nxeNFIYB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7492f7eb-9a53-4a74-c7b0-dc67b1a06bf0"
      },
      "source": [
        "import os # libreria de manejo del sistema operativo\n",
        "#instalar automaticamente la version deseadda de spark\n",
        "os.system(f\"wget -q https://www-us.apache.org/dist/spark/{ver_spark}/{ver_spark}-bin-hadoop2.7.tgz\")\n",
        "os.system(f\"tar xf {ver_spark}-bin-hadoop2.7.tgz\")\n",
        "# instalar pyspark\n",
        "!pip install -q pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 217.8MB 63kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 53.8MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5zJWYOBHX37"
      },
      "source": [
        "## Definir variables de entorno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0tFuu8VHWcm"
      },
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{ver_spark}-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCGyEt25Hh-P"
      },
      "source": [
        "# Cargar pyspark en el sistema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtzN28KKIY82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "4d5bce88-7dfc-4d2b-d475-180dac7168c4"
      },
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Test_spark\").master(\"local[2]\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 78kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 20.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=813d1abf5f5cd19b7cc34f93e5661f6b7c0f823729b9549468bd34af54cbd2a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cf21c3300cd2:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[2]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Test_spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f3fe2488290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IImWwtaEKHXB"
      },
      "source": [
        "# Ejemplo de Uso de pyspark\n",
        "\n",
        "Leer archivo de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-2gRfS-KQyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7311015-2e13-4039-a1f5-38cf8da27c2d"
      },
      "source": [
        "archivo = '/content/ventas_prediccion.csv'\n",
        "df_spark = spark.read.csv(archivo, inferSchema=True, header=True)\n",
        "\n",
        "# imprimir tipo de archivo\n",
        "print(type(df_spark))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMbYCf5hPZe2"
      },
      "source": [
        "¿Numero de registros en el dataframe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrbNpoUCNYN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ffd733-f501-4e5f-e0b6-a7ea5b96ddf9"
      },
      "source": [
        "df_spark.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "603"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13rl3pyARA28"
      },
      "source": [
        "Estructura del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sb_is4QLeB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f0f209-6b52-4011-ebf7-397418897f26"
      },
      "source": [
        "df_spark.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- 2017-01-02: string (nullable = true)\n",
            " |-- 236: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjN5k0S_PfaB"
      },
      "source": [
        "¿Nombre de las Columnas de dataframe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLa8YbIPMWhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ac8518-7953-4d1a-8857-30aef3236eb0"
      },
      "source": [
        "df_spark.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2017-01-02', '236']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zTVwhBBPkKQ"
      },
      "source": [
        "Ver los primeros 20 registros del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVUfc5nyLpxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4228136b-a6ee-4ef6-9907-147040b63cd8"
      },
      "source": [
        "df_spark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+\n",
            "|2017-01-02|236|\n",
            "+----------+---+\n",
            "|2017-01-03|237|\n",
            "|2017-01-04|290|\n",
            "|2017-01-05|221|\n",
            "|2017-01-07|128|\n",
            "|2017-01-09|293|\n",
            "|2017-01-10|260|\n",
            "|2017-01-11|204|\n",
            "|2017-01-12|157|\n",
            "|2017-01-13|180|\n",
            "|2017-01-14|147|\n",
            "|2017-01-16|228|\n",
            "|2017-01-17|239|\n",
            "|2017-01-18|193|\n",
            "|2017-01-19|197|\n",
            "|2017-01-20|189|\n",
            "|2017-01-21|153|\n",
            "|2017-01-22| 58|\n",
            "|2017-01-23|193|\n",
            "|2017-01-24|232|\n",
            "|2017-01-25|206|\n",
            "+----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPsuLzLfQAfj"
      },
      "source": [
        "## Descricipcion Estadistica del dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_u_DrLfMwKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "98e0c8d6-7003-4714-de6c-a7ef70b8905e"
      },
      "source": [
        "df_spark.describe().toPandas().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>summary</th>\n",
              "      <td>count</td>\n",
              "      <td>mean</td>\n",
              "      <td>stddev</td>\n",
              "      <td>min</td>\n",
              "      <td>max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-02</th>\n",
              "      <td>603</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>2018-11-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>603</td>\n",
              "      <td>215.90215588723052</td>\n",
              "      <td>75.10815266449659</td>\n",
              "      <td>51</td>\n",
              "      <td>591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0                   1  ...           3           4\n",
              "summary     count                mean  ...         min         max\n",
              "2017-01-02    603                None  ...  2017-01-03  2018-11-30\n",
              "236           603  215.90215588723052  ...          51         591\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "lNx2_S1G2qKq",
        "outputId": "dd69f7f4-1985-46b5-b9bc-685e5395d9de"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "df_spark.describe().toPandas().transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>summary</th>\n",
              "      <td>count</td>\n",
              "      <td>mean</td>\n",
              "      <td>stddev</td>\n",
              "      <td>min</td>\n",
              "      <td>max</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-02</th>\n",
              "      <td>603</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>2017-01-03</td>\n",
              "      <td>2018-11-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>603</td>\n",
              "      <td>215.90215588723052</td>\n",
              "      <td>75.10815266449659</td>\n",
              "      <td>51</td>\n",
              "      <td>591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                0                   1  ...           3           4\n",
              "summary     count                mean  ...         min         max\n",
              "2017-01-02    603                None  ...  2017-01-03  2018-11-30\n",
              "236           603  215.90215588723052  ...          51         591\n",
              "\n",
              "[3 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5BDOMIh3O01",
        "outputId": "fd3b5a15-cce4-4754-8922-56205f7195ab"
      },
      "source": [
        "df_spark.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+\n",
            "|2017-01-02|236|\n",
            "+----------+---+\n",
            "|2017-01-03|237|\n",
            "|2017-01-04|290|\n",
            "|2017-01-05|221|\n",
            "|2017-01-07|128|\n",
            "|2017-01-09|293|\n",
            "|2017-01-10|260|\n",
            "|2017-01-11|204|\n",
            "|2017-01-12|157|\n",
            "|2017-01-13|180|\n",
            "|2017-01-14|147|\n",
            "|2017-01-16|228|\n",
            "|2017-01-17|239|\n",
            "|2017-01-18|193|\n",
            "|2017-01-19|197|\n",
            "|2017-01-20|189|\n",
            "|2017-01-21|153|\n",
            "|2017-01-22| 58|\n",
            "|2017-01-23|193|\n",
            "|2017-01-24|232|\n",
            "|2017-01-25|206|\n",
            "+----------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7TL6s-84X4O",
        "outputId": "a4c56e4e-45a9-49c0-b1cb-5aa65882c43c"
      },
      "source": [
        "archivo = '/content/ventas_prediccion.csv'\r\n",
        "df_spark = spark.read.csv(archivo, inferSchema=True, header=True)\r\n",
        "\r\n",
        "# imprimir tipo de archivo\r\n",
        "print(type(df_spark))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMoLSdwRRFh_"
      },
      "source": [
        "Descripcion estadistica de una sola columna ('median_house_value')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcukuEwtFf0s"
      },
      "source": [
        "De esta forma se puede instalar automaticamente spark en google colab y hacer uno de el de forma gratis.\n",
        "\n",
        "En la version gratis solo se cuenta con una CPU si se quiere aumentar la capacidad de procesamiento es necesario pagar."
      ]
    }
  ]
}